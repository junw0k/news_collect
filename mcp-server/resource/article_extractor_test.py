import os
import urllib.request
import urllib.parse
import json
import re
from typing import Dict, Any, Optional, List # â¬…ï¸ Listì™€ Dict, Optional ì¶”ê°€
from bs4 import BeautifulSoup, Tag
import requests

# í™˜ê²½ ë³€ìˆ˜ì—ì„œ Client IDì™€ Secretì„ ê°€ì ¸ì˜µë‹ˆë‹¤. 
# (ì‹¤ì œ ì‹¤í–‰ ì‹œ í„°ë¯¸ë„ì—ì„œ export ë˜ëŠ” .env íŒŒì¼ë¡œ ì„¤ì • í•„ìš”)
#CLIENT_ID = os.environ.get("NAVER_CLIENT_ID", "YOUR_CLIENT_ID")
#CLIENT_SECRET = os.environ.get("NAVER_CLIENT_SECRET", "YOUR_CLIENT_SECRET")
# ğŸ’¡ ë§Œì•½ í™˜ê²½ ë³€ìˆ˜ê°€ ì•„ë‹Œ íŒŒì¼ ë‚´ì˜ ì„ì‹œ ê°’ìœ¼ë¡œ í…ŒìŠ¤íŠ¸í•˜ë ¤ë©´,
CLIENT_ID = 'CkaoiuDQTdAhLKTc0LqX'
CLIENT_SECRET = '7DqMOLcXMi'


def fetch_naver_news_items(topic: str, display: int = 3, sort: str = 'sim') -> Optional[Dict[str, Any]]:
    """
    ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ APIë¥¼ í˜¸ì¶œí•˜ì—¬ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    if not CLIENT_ID or not CLIENT_SECRET:
        print("Error: NAVER_CLIENT_ID or NAVER_CLIENT_SECRET not set.")
        return None

    # 1. API URL ë° ê²€ìƒ‰ì–´ ì„¤ì •
    encText = urllib.parse.quote(topic)
    # ğŸš¨ ë‰´ìŠ¤ ê²€ìƒ‰ API URLë¡œ ë³€ê²½ (blog -> news)
    url = f"https://openapi.naver.com/v1/search/news.json?query={encText}&display={display}&sort={sort}" 

    # 2. HTTP ìš”ì²­ ê°ì²´ ìƒì„± ë° í—¤ë” ì¶”ê°€
    request = urllib.request.Request(url)
    request.add_header("X-Naver-Client-Id", CLIENT_ID)
    request.add_header("X-Naver-Client-Secret", CLIENT_SECRET)

    try:
        # 3. API í˜¸ì¶œ ë° ì‘ë‹µ ì²˜ë¦¬
        with urllib.request.urlopen(request) as response:
            rescode = response.getcode()
            
            if rescode == 200:
                response_body = response.read()
                # ğŸ’¡ JSON í˜•íƒœë¡œ ë””ì½”ë”©í•˜ì—¬ ë”•ì…”ë„ˆë¦¬ ë°˜í™˜
                return json.loads(response_body.decode('utf-8'))
            else:
                # HTTP ì˜¤ë¥˜ ì½”ë“œ ì¶œë ¥
                print(f"Error Code: {rescode}")
                # API ì˜¤ë¥˜ ì‘ë‹µ ë³¸ë¬¸ì„ ì½ì–´ ì„¸ë¶€ ì •ë³´ ì¶œë ¥ ì‹œë„
                error_body = response.read().decode('utf-8')
                print(f"Error Detail: {error_body}")
                return None
                
    except urllib.error.URLError as e:
        print(f"URL Error: Could not connect to API. {e}")
        return None
    except Exception as e:
        print(f"An unexpected error occurred: {e}")
        return None

# ====================================================================
# A. í¬ë¡¤ë§ í•¨ìˆ˜ (ë³¸ë¬¸ ì¶”ì¶œ)
# ====================================================================

def fetch_article_text(url: str) -> str:
    """
    ì£¼ì–´ì§„ URLì—ì„œ ê¸°ì‚¬ ë³¸ë¬¸ì„ ì¶”ì¶œí•©ë‹ˆë‹¤. (ì´ì „ì— êµ¬í˜„í–ˆë˜ í¬ë¡¤ë§ ë¡œì§)
    """
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (CollectorBot)'}
        # requestsë¥¼ ì‚¬ìš©í•˜ì—¬ ë™ê¸° HTTP ìš”ì²­
        response = requests.get(
            url, 
            headers=headers, 
            timeout=15.0, 
            allow_redirects=True
        )
        response.raise_for_status() 

    except requests.exceptions.RequestException as e:
        # print(f"[Collector] Request Error for {url}: {e}") # ë””ë²„ê·¸ ì¶œë ¥
        return ""

    html = response.text
    soup = BeautifulSoup(html, 'html.parser')

    # ì¡ìŒ ì œê±° (Script, Style, ê´‘ê³ , ê³µìœ  ë²„íŠ¼ ë“±)
    noise_selectors = [
        'script', 'style', 'noscript', 'iframe', 'svg', 'form', 'header', 
        'footer', 'nav', 'aside',
        '[class*="ad"]', '[id*="ad"]', '.sns', '.share', '.copyright', 
        '.related', '.recommend', '.banner'
    ]
    for selector in noise_selectors:
        for tag in soup.select(selector):
            if isinstance(tag, Tag):
                tag.decompose()

    # ë³¸ë¬¸ í›„ë³´ ì„ íƒì (ë„¤ì´ë²„ ë‰´ìŠ¤ í˜ì´ì§€ êµ¬ì¡°ì— ë§ì¶˜ íœ´ë¦¬ìŠ¤í‹±)
    candidates = [
        'article', '#newsct_article', '.newsct_article', 
        '#dic_area', '.article_body', '#articeBody', 
        '.news_end', '.article_content', '#contents', 
        '#content', '.content',
    ]

    for sel in candidates:
        element = soup.select_one(sel)
        if element:
            # í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ê³µë°± ì •ë¦¬
            text = re.sub(r'\s+', ' ', element.get_text()).strip()
            # 200ì ì´ìƒì¼ ê²½ìš° ë³¸ë¬¸ìœ¼ë¡œ ì¸ì •
            if text and len(text) > 200:
                return text

    return "" # ë³¸ë¬¸ì„ ì°¾ì§€ ëª»í•˜ê±°ë‚˜ 200ì ë¯¸ë§Œì¼ ê²½ìš° ë¹ˆ ë¬¸ìì—´ ë°˜í™˜

# ====================================================================
# B. API í˜¸ì¶œ í•¨ìˆ˜ (URL ìˆ˜ì§‘)
# ====================================================================

def fetch_naver_news_items(topic: str, display: int = 3, sort: str = 'sim') -> List[Dict[str, str]]:
    """
    ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ APIë¥¼ í˜¸ì¶œí•˜ì—¬ URLê³¼ ì œëª© ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    # (API í˜¸ì¶œ ë¡œì§ì€ ì´ì „ ë‹µë³€ì˜ ì½”ë“œì™€ ë™ì¼)
    if not CLIENT_ID or not CLIENT_SECRET:
        print("Error: NAVER_CLIENT_ID or NAVER_CLIENT_SECRET not set.")
        return []

    encText = urllib.parse.quote(topic)
    url = f"https://openapi.naver.com/v1/search/news.json?query={encText}&display={display}&sort={sort}" 

    request = urllib.request.Request(url)
    request.add_header("X-Naver-Client-Id", CLIENT_ID)
    request.add_header("X-Naver-Client-Secret", CLIENT_SECRET)

    try:
        with urllib.request.urlopen(request) as response:
            if response.getcode() == 200:
                data = json.loads(response.read().decode('utf-8'))
                
                # ì¶”ì¶œëœ í•­ëª©ë§Œ ì •ë¦¬í•˜ì—¬ ë°˜í™˜
                return [
                    {
                        'title': re.sub(r'<b>|</b>', '', item.get('title', '')), # <b> íƒœê·¸ ì œê±°
                        'url': item.get('originallink') or item.get('link')
                    }
                    for item in data.get('items', [])
                ]
            else:
                print(f"API Error Code: {response.getcode()}")
                return []
    except Exception as e:
        print(f"API Connection Error: {e}")
        return []

# ====================================================================
# C. ë©”ì¸ ì‹¤í–‰ ë° í†µí•©
# ====================================================================

if __name__ == '__main__':
    SEARCH_TOPIC = "ì¸ê³µì§€ëŠ¥ íŠ¸ë Œë“œ"
    
    print(f"1. ë„¤ì´ë²„ APIë¡œ '{SEARCH_TOPIC}' ê´€ë ¨ URL 5ê°œ ê²€ìƒ‰ ì¤‘...")
    
    # 1. API í˜¸ì¶œë¡œ URLê³¼ ì œëª© ëª©ë¡ íšë“
    news_list = fetch_naver_news_items(SEARCH_TOPIC, display=3)

    if not news_list:
        print("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ê±°ë‚˜ API í˜¸ì¶œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
    else:
        print(f"2. ê²€ìƒ‰ëœ {len(news_list)}ê°œ ê¸°ì‚¬ì˜ ë³¸ë¬¸ì„ í¬ë¡¤ë§ ì¤‘...")
        
        # 2. ê° ê¸°ì‚¬ë¥¼ ìˆœíšŒí•˜ë©° ë³¸ë¬¸ í¬ë¡¤ë§
        crawled_articles = []
        for i, item in enumerate(news_list):
            
            # ğŸš¨ í¬ë¡¤ë§ í•¨ìˆ˜ í˜¸ì¶œ ğŸš¨
            text = fetch_article_text(item['url']) 
            
            crawled_articles.append({
                'title': item['title'],
                'url': item['url'],
                'text': text
            })
            print(f"   -> {i+1}. '{item['title'][:20]}...' ë³¸ë¬¸ ì¶”ì¶œ ì™„ë£Œ (ê¸¸ì´: {len(text)})")

        # 3. ê²°ê³¼ ì¶œë ¥
        print("\n" + "=" * 50)
        print(f"=== ìµœì¢… í†µí•© ê²°ê³¼ (ì£¼ì œ: {SEARCH_TOPIC}) ===")
        print("=" * 50)
        
        for article in crawled_articles:
            print(f"\n[ê¸°ì‚¬ ì œëª©]: {article['title']}")
            print(f"[URL]: {article['url']}")
            
            if article['text']:
                # ì¶”ì¶œëœ ë³¸ë¬¸ì˜ 500ìê¹Œì§€ë§Œ ì¶œë ¥
                # body_preview ëŒ€ì‹  ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.
                #body_preview = article['text'][:1000]
                #print(f"[ë³¸ë¬¸ (500ì ë¯¸ë¦¬ë³´ê¸°)]:\n{body_preview}...")
                print(f"[ë³¸ë¬¸ (ì „ì²´ ë‚´ìš©)]:\n{article['text']}")
            else:
                print("[ë³¸ë¬¸]: âŒ í¬ë¡¤ë§ ì‹¤íŒ¨ ë˜ëŠ” ë³¸ë¬¸ 200ì ë¯¸ë§Œ.")
        print("=" * 50)



